{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **General Description**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# Datasets Used:\n",
        "- PathMnist : a large 9 class pathology image dataset with 28x28 images and a single channel\n",
        "- ChestMNIST : a chest X-ray dataset with 14 possible desease labels. each image can have multiple labels / diseases.\n",
        "# Pre-Processing Steps:\n",
        "- resized the images to 224x224\n",
        "- converted to 3 channels where needed\n",
        "- normalized with simple mean = 0,5 and std = 0.5\n",
        "- loaded into batches of 256 for better training speed ( it was very slow)\n",
        "- i trained on subsets of the dataser ( 10k for pathMNIST, 8k for ChestMNIST) to have run in a reasonable time frame ( i have google collab gpu usage limits & need them for other projects)\n",
        "\n",
        "# other info\n",
        "\n",
        "- I used Accuracy and f1 score for my performance metrics\n",
        " - i ran ResNet-18, VGG-16, ViT-Base, and DINO ViT-Small\n",
        " - i trained only the model heads\n",
        " - i used 1 epoch per module for gpu & time limits\n",
        " - optimizer was Adam, learning rate 1e-3\n"
      ],
      "metadata": {
        "id": "RpXlzw_I3OrI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports, configuration, and dataset metadata\n"
      ],
      "metadata": {
        "id": "UJTxmygnL1UB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9o93vywHE5Q",
        "outputId": "48e3a98c-694c-46e8-b502-c8170ab1be5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available: True\n"
          ]
        }
      ],
      "source": [
        "!pip install -q medmnist timm torchinfo\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torchvision import transforms\n",
        "import timm\n",
        "import medmnist\n",
        "from medmnist import INFO\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.stats as st\n",
        "import time\n",
        "\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Hyperparameters tuned for speed\n",
        "BATCH_SIZE = 256\n",
        "EPOCHS = 1           # increase to 2 if you have time\n",
        "NUM_WORKERS = 2\n",
        "\n",
        "DATASET_1 = \"pathmnist\"\n",
        "DATASET_2 = \"chestmnist\"\n",
        "\n",
        "info1 = INFO[DATASET_1]\n",
        "info2 = INFO[DATASET_2]\n",
        "\n",
        "DataClass1 = getattr(medmnist, info1['python_class'])\n",
        "DataClass2 = getattr(medmnist, info2['python_class'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  MedMNIST dataset loading and preprocessing (PathMNIST + ChestMNIST)"
      ],
      "metadata": {
        "id": "waXDoA8uMGHf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 224x224 + simple normalization for all pretrained models\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "])\n",
        "\n",
        "# ----- Dataset 1: PathMNIST -----\n",
        "train1 = DataClass1(split='train', transform=transform, download=True)\n",
        "test1  = DataClass1(split='test',  transform=transform, download=True)\n",
        "\n",
        "# Optional: use a subset for faster training; set to None for full dataset\n",
        "SUBSET_TRAIN1 = 10000   # e.g., 10k samples; change to None for full\n",
        "\n",
        "if SUBSET_TRAIN1 is not None:\n",
        "    idx1 = np.random.choice(len(train1), SUBSET_TRAIN1, replace=False)\n",
        "    train1_sub = Subset(train1, idx1)\n",
        "else:\n",
        "    train1_sub = train1\n",
        "\n",
        "train_loader1 = DataLoader(\n",
        "    train1_sub,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "test_loader1 = DataLoader(\n",
        "    test1,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "# ----- Dataset 2: ChestMNIST -----\n",
        "train2 = DataClass2(split='train', transform=transform, download=True)\n",
        "test2  = DataClass2(split='test',  transform=transform, download=True)\n",
        "\n",
        "SUBSET_TRAIN2 = 8000    # keep this smaller; change to None for full\n",
        "\n",
        "if SUBSET_TRAIN2 is not None:\n",
        "    idx2 = np.random.choice(len(train2), SUBSET_TRAIN2, replace=False)\n",
        "    train2_sub = Subset(train2, idx2)\n",
        "else:\n",
        "    train2_sub = train2\n",
        "\n",
        "train_loader2 = DataLoader(\n",
        "    train2_sub,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "test_loader2 = DataLoader(\n",
        "    test2,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nymWezTuRKYW",
        "outputId": "8e0cd15b-bbf5-4f9f-c4cb-3fa6d8b49d17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 206M/206M [02:57<00:00, 1.16MB/s]\n",
            "100%|██████████| 82.8M/82.8M [00:22<00:00, 3.75MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training and evaluation utilities"
      ],
      "metadata": {
        "id": "HZn9-IyDMLpo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----- Determine class count safely for dataset 1 -----\n",
        "num_classes1 = (\n",
        "    info1.get(\"num_classes\") or\n",
        "    info1.get(\"n_classes\") or\n",
        "    len(info1.get(\"label\", [])) or\n",
        "    len(info1.get(\"labels\", []))\n",
        ")\n",
        "\n",
        "if not num_classes1:\n",
        "    try:\n",
        "        num_classes1 = train1.num_classes\n",
        "    except:\n",
        "        num_classes1 = len(np.unique(train1.labels))\n",
        "\n",
        "print(\"Dataset 1 (PathMNIST) classes:\", num_classes1)\n",
        "\n",
        "# For dataset 2\n",
        "num_classes2 = (\n",
        "    info2.get(\"num_classes\") or\n",
        "    info2.get(\"n_classes\") or\n",
        "    len(info2.get(\"label\", [])) or\n",
        "    len(info2.get(\"labels\", []))\n",
        ")\n",
        "\n",
        "if not num_classes2:\n",
        "    try:\n",
        "        num_classes2 = train2.num_classes\n",
        "    except:\n",
        "        num_classes2 = len(np.unique(train2.labels))\n",
        "\n",
        "print(\"Dataset 2 (ChestMNIST) classes:\", num_classes2)\n",
        "\n",
        "\n",
        "def train_model(model, loader, epochs=EPOCHS):\n",
        "    model.to(device)\n",
        "    params = filter(lambda p: p.requires_grad, model.parameters())\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(params, lr=1e-3)\n",
        "\n",
        "    for ep in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for x, y in loader:\n",
        "            x = x.to(device, non_blocking=True)\n",
        "            y = y.squeeze().to(device, non_blocking=True)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            pred = model(x)\n",
        "            loss = criterion(pred, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item() * x.size(0)\n",
        "\n",
        "        epoch_loss = running_loss / len(loader.dataset)\n",
        "        print(f\"Epoch {ep+1}/{epochs} - loss: {epoch_loss:.4f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def evaluate(model, loader):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x = x.to(device, non_blocking=True)\n",
        "            pred = model(x)\n",
        "            all_preds.extend(pred.cpu().argmax(1).numpy())\n",
        "            all_labels.extend(y.squeeze().numpy())\n",
        "\n",
        "    all_preds = np.array(all_preds)\n",
        "    all_labels = np.array(all_labels)\n",
        "\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    f1  = f1_score(all_labels, all_preds, average=\"macro\")\n",
        "    cm  = confusion_matrix(all_labels, all_preds)\n",
        "    return acc, f1, cm, all_preds\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ny8HeUw9RiCG",
        "outputId": "86ec5825-28b1-4622-8231-79e3827572c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset 1 (PathMNIST) classes: 9\n",
            "Dataset 2 (ChestMNIST) classes: 14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-trained model builders (ResNet18, VGG16, ViT-Base, DINO-ViT)"
      ],
      "metadata": {
        "id": "964dkP9ZMTfo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_resnet(num_classes):\n",
        "    model = timm.create_model(\"resnet18\", pretrained=True)\n",
        "    for name, param in model.named_parameters():\n",
        "        if \"fc\" not in name:\n",
        "            param.requires_grad = False\n",
        "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "    return model\n",
        "\n",
        "def build_vgg(num_classes):\n",
        "    # Let timm build the correct classifier\n",
        "    model = timm.create_model(\"vgg16\", pretrained=True, num_classes=num_classes)\n",
        "\n",
        "    # Freeze backbone layers\n",
        "    for name, param in model.named_parameters():\n",
        "        # timm VGG uses \"head\" for the classifier module\n",
        "        if \"head\" not in name:\n",
        "            param.requires_grad = False\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "def build_vit(num_classes):\n",
        "    model = timm.create_model(\"vit_base_patch16_224\", pretrained=True, num_classes=num_classes)\n",
        "    for name, param in model.named_parameters():\n",
        "        if \"head\" not in name:\n",
        "            param.requires_grad = False\n",
        "    return model\n",
        "\n",
        "def build_dino(num_classes):\n",
        "    model = timm.create_model(\"vit_small_patch16_224.dino\", pretrained=True)\n",
        "\n",
        "    # Freeze all layers except head\n",
        "    for name, param in model.named_parameters():\n",
        "        if \"head\" not in name:\n",
        "            param.requires_grad = False\n",
        "\n",
        "    # Replace head properly using model.num_features\n",
        "    model.head = nn.Linear(model.num_features, num_classes)\n",
        "\n",
        "    return model\n",
        "\n"
      ],
      "metadata": {
        "id": "pv3QXpAdXRGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tuning individual models on PathMNIST"
      ],
      "metadata": {
        "id": "aS8XpCZ5MZOB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Dataset 1: PathMNIST =====\n",
        "print(\"=== DATASET 1: PathMNIST ===\")\n",
        "\n",
        "# ResNet\n",
        "print(\"\\n=== Training ResNet-18 (PathMNIST) ===\")\n",
        "start = time.time()\n",
        "resnet1 = build_resnet(num_classes1)\n",
        "resnet1 = train_model(resnet1, train_loader1)\n",
        "acc_res1, f1_res1, cm_res1, preds_res1 = evaluate(resnet1, test_loader1)\n",
        "print(f\"ResNet-18 done in {time.time()-start:.2f} s\")\n",
        "print(\"Accuracy:\", acc_res1, \"| F1:\", f1_res1)\n",
        "\n",
        "# VGG-16\n",
        "print(\"\\n=== Training VGG-16 (PathMNIST) ===\")\n",
        "start = time.time()\n",
        "vgg1 = build_vgg(num_classes1)\n",
        "vgg1 = train_model(vgg1, train_loader1)\n",
        "acc_vgg1, f1_vgg1, cm_vgg1, preds_vgg1 = evaluate(vgg1, test_loader1)\n",
        "print(f\"VGG-16 done in {time.time()-start:.2f} s\")\n",
        "print(\"Accuracy:\", acc_vgg1, \"| F1:\", f1_vgg1)\n",
        "\n",
        "# ViT\n",
        "print(\"\\n=== Training ViT-Base (PathMNIST) ===\")\n",
        "start = time.time()\n",
        "vit1 = build_vit(num_classes1)\n",
        "vit1 = train_model(vit1, train_loader1)\n",
        "acc_vit1, f1_vit1, cm_vit1, preds_vit1 = evaluate(vit1, test_loader1)\n",
        "print(f\"ViT-Base done in {time.time()-start:.2f} s\")\n",
        "print(\"Accuracy:\", acc_vit1, \"| F1:\", f1_vit1)\n",
        "\n",
        "# DINO\n",
        "print(\"\\n=== Training DINO ViT-Small (PathMNIST) ===\")\n",
        "start = time.time()\n",
        "dino1 = build_dino(num_classes1)\n",
        "dino1 = train_model(dino1, train_loader1)\n",
        "acc_dino1, f1_dino1, cm_dino1, preds_dino1 = evaluate(dino1, test_loader1)\n",
        "print(f\"DINO done in {time.time()-start:.2f} s\")\n",
        "print(\"Accuracy:\", acc_dino1, \"| F1:\", f1_dino1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yq8RgB5SXWJ5",
        "outputId": "14e4e63b-d66e-49de-9973-9d2fa7b16c6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== DATASET 1: PathMNIST ===\n",
            "\n",
            "=== Training ResNet-18 (PathMNIST) ===\n",
            "Epoch 1/1 - loss: 1.8817\n",
            "ResNet-18 done in 36.28 s\n",
            "Accuracy: 0.5916434540389972 | F1: 0.46720830040490086\n",
            "\n",
            "=== Training VGG-16 (PathMNIST) ===\n",
            "Epoch 1/1 - loss: 1.0577\n",
            "VGG-16 done in 117.81 s\n",
            "Accuracy: 0.8064066852367688 | F1: 0.7468896132710968\n",
            "\n",
            "=== Training ViT-Base (PathMNIST) ===\n",
            "Epoch 1/1 - loss: 1.0193\n",
            "ViT-Base done in 211.44 s\n",
            "Accuracy: 0.7987465181058496 | F1: 0.7377623114521275\n",
            "\n",
            "=== Training DINO ViT-Small (PathMNIST) ===\n",
            "Epoch 1/1 - loss: 1.8099\n",
            "DINO done in 61.25 s\n",
            "Accuracy: 0.8231197771587744 | F1: 0.7493981812862142\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ensemble learning on PathMNIST (majority vote, weighted soft-voting, stacking)"
      ],
      "metadata": {
        "id": "hllDxJjVMdEx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels1 = test1.labels.squeeze()\n",
        "\n",
        "# Majority voting\n",
        "all_votes1 = np.vstack([preds_res1, preds_vgg1, preds_vit1, preds_dino1])\n",
        "maj_preds1 = st.mode(all_votes1, axis=0, keepdims=True).mode.squeeze()\n",
        "\n",
        "acc_maj1 = accuracy_score(labels1, maj_preds1)\n",
        "f1_maj1  = f1_score(labels1, maj_preds1, average=\"macro\")\n",
        "cm_maj1  = confusion_matrix(labels1, maj_preds1)\n",
        "\n",
        "# Weighted averaging (soft voting)\n",
        "def get_soft(model, loader):\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    soft = []\n",
        "    with torch.no_grad():\n",
        "        for x, _ in loader:\n",
        "            x = x.to(device, non_blocking=True)\n",
        "            probs = nn.Softmax(dim=1)(model(x)).cpu().numpy()\n",
        "            soft.append(probs)\n",
        "    return np.vstack(soft)\n",
        "\n",
        "soft_res1  = get_soft(resnet1, test_loader1)\n",
        "soft_vgg1  = get_soft(vgg1, test_loader1)\n",
        "soft_vit1  = get_soft(vit1, test_loader1)\n",
        "soft_dino1 = get_soft(dino1, test_loader1)\n",
        "\n",
        "weights1 = np.array([acc_res1, acc_vgg1, acc_vit1, acc_dino1])\n",
        "weights1 = weights1 / weights1.sum()\n",
        "\n",
        "weighted_soft1 = (\n",
        "    weights1[0]*soft_res1 +\n",
        "    weights1[1]*soft_vgg1 +\n",
        "    weights1[2]*soft_vit1 +\n",
        "    weights1[3]*soft_dino1\n",
        ")\n",
        "\n",
        "weighted_preds1 = weighted_soft1.argmax(axis=1)\n",
        "\n",
        "acc_w1 = accuracy_score(labels1, weighted_preds1)\n",
        "f1_w1  = f1_score(labels1, weighted_preds1, average=\"macro\")\n",
        "cm_w1  = confusion_matrix(labels1, weighted_preds1)\n",
        "\n",
        "# Stacking\n",
        "stack_X1 = np.vstack([\n",
        "    preds_res1,\n",
        "    preds_vgg1,\n",
        "    preds_vit1,\n",
        "    preds_dino1\n",
        "]).T\n",
        "\n",
        "stack_y1 = labels1\n",
        "\n",
        "meta1 = LogisticRegression(max_iter=500)\n",
        "meta1.fit(stack_X1, stack_y1)\n",
        "stack_preds1 = meta1.predict(stack_X1)\n",
        "\n",
        "acc_s1 = accuracy_score(stack_y1, stack_preds1)\n",
        "f1_s1  = f1_score(stack_y1, stack_preds1, average=\"macro\")\n",
        "cm_s1  = confusion_matrix(stack_y1, stack_preds1)\n"
      ],
      "metadata": {
        "id": "qaVLVWjtXZiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transfer learning on second dataset: ChestMNIST (ResNet18)"
      ],
      "metadata": {
        "id": "Yhe1snKlMjBY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== Training DINO ViT-Small ===\")\n",
        "start = time.time()\n",
        "\n",
        "# Build DINO model properly\n",
        "dino = timm.create_model(\"vit_small_patch16_224.dino\", pretrained=True)\n",
        "\n",
        "# Freeze everything except head\n",
        "for name, param in dino.named_parameters():\n",
        "    if \"head\" not in name:\n",
        "        param.requires_grad = False\n",
        "\n",
        "# Replace head classifier\n",
        "dino.head = nn.Linear(dino.num_features, num_classes1)\n",
        "\n",
        "print(\"DINO: starting training...\")\n",
        "dino = train_model(dino, train_loader1)\n",
        "print(\"DINO: evaluating...\")\n",
        "acc_dino1, f1_dino1, cm_dino1, preds_dino1 = evaluate(dino, test_loader1)\n",
        "\n",
        "end = time.time()\n",
        "print(f\"DINO done in {end-start:.2f} seconds\")\n",
        "print(\"Accuracy:\", acc_dino1, \"| F1:\", f1_dino1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pqqJ-qwbXfH4",
        "outputId": "ed678f49-c199-4b4b-d65d-6a6f1446815f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Training DINO ViT-Small ===\n",
            "DINO: starting training...\n",
            "Epoch 1/1 - loss: 1.2776\n",
            "DINO: evaluating...\n",
            "DINO done in 65.42 seconds\n",
            "Accuracy: 0.834958217270195 | F1: 0.7762318564602073\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ChestMNIST preprocessing + ResNet18 training"
      ],
      "metadata": {
        "id": "dqP5iX9-LqYH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== CHESTMNIST PREPROCESSING ===\")\n",
        "\n",
        "# 1) Reload ChestMNIST with correct transform (grayscale -> 3-channel RGB)\n",
        "transform_chest = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.Grayscale(num_output_channels=3),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
        "                         std=[0.5, 0.5, 0.5]),\n",
        "])\n",
        "\n",
        "train2 = DataClass2(split='train', transform=transform_chest, download=True)\n",
        "test2  = DataClass2(split='test',  transform=transform_chest, download=True)\n",
        "\n",
        "# 2) Convert multi-label (14-d) ChestMNIST labels -> single class index\n",
        "def chest_single_label(dataset):\n",
        "    labels = []\n",
        "    for y in dataset.labels:\n",
        "        y = y.astype(int)\n",
        "        if y.sum() == 0:\n",
        "            labels.append(0)\n",
        "        else:\n",
        "            labels.append(np.argmax(y))\n",
        "    dataset.labels = np.array(labels)\n",
        "    return dataset\n",
        "\n",
        "train2 = chest_single_label(train2)\n",
        "test2  = chest_single_label(test2)\n",
        "\n",
        "# 3) DataLoaders for ChestMNIST\n",
        "train_loader2 = DataLoader(\n",
        "    train2,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "test_loader2 = DataLoader(\n",
        "    test2,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "# 4) Derive number of classes from processed labels\n",
        "num_classes2 = len(np.unique(train2.labels))\n",
        "print(\"ChestMNIST num_classes (after collapsing):\", num_classes2)\n",
        "\n",
        "# 5) Reuse ResNet builder (if not already defined exactly like this above)\n",
        "def build_resnet(num_classes):\n",
        "    model = timm.create_model(\"resnet18\", pretrained=True)\n",
        "    # freeze backbone\n",
        "    for name, param in model.named_parameters():\n",
        "        if \"fc\" not in name:\n",
        "            param.requires_grad = False\n",
        "    # new classification head\n",
        "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "    return model\n",
        "\n",
        "print(\"\\n=== TRAINING CHESTMNIST RESNET-18 ===\")\n",
        "start = time.time()\n",
        "resnet2 = build_resnet(num_classes2)\n",
        "resnet2 = train_model(resnet2, train_loader2)\n",
        "acc_res2, f1_res2, cm_res2, preds_res2 = evaluate(resnet2, test_loader2)\n",
        "print(f\"ResNet-18 (ChestMNIST) done in {time.time()-start:.2f} s\")\n",
        "print(\"Accuracy:\", acc_res2, \"| F1:\", f1_res2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_NBlNTyRxr8",
        "outputId": "e95e477d-caa6-46e8-f17f-2b46522c72db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== CHESTMNIST PREPROCESSING ===\n",
            "ChestMNIST num_classes (after collapsing): 14\n",
            "\n",
            "=== TRAINING CHESTMNIST RESNET-18 ===\n",
            "Epoch 1/1 - loss: 1.3987\n",
            "ResNet-18 (ChestMNIST) done in 182.95 s\n",
            "Accuracy: 0.6395934560691838 | F1: 0.05572753013007492\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final results summary + confusion matrices"
      ],
      "metadata": {
        "id": "hJSa71I2LjEx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"\\n================ FINAL RESULTS ================\")\n",
        "\n",
        "print(\"\\n=== PATHMNIST INDIVIDUAL MODELS ===\")\n",
        "print(\"ResNet-18 :\", acc_res1,  f1_res1)\n",
        "print(\"VGG-16    :\", acc_vgg1,  f1_vgg1)\n",
        "print(\"ViT-Base  :\", acc_vit1,  f1_vit1)\n",
        "print(\"DINO ViT  :\", acc_dino1, f1_dino1)\n",
        "\n",
        "print(\"\\n=== PATHMNIST ENSEMBLES ===\")\n",
        "print(\"Majority Voting :\", acc_maj1, f1_maj1)\n",
        "print(\"Weighted Avg    :\", acc_w1,  f1_w1)\n",
        "print(\"Stacking        :\", acc_s1,  f1_s1)\n",
        "\n",
        "print(\"\\n=== CHESTMNIST (ResNet-18 only) ===\")\n",
        "print(\"ResNet-18 :\", acc_res2, f1_res2)\n",
        "\n",
        "print(\"\\n================ END METRICS ================\\n\")\n",
        "\n",
        "\n",
        "# --- Confusion matrix plotting helper ---\n",
        "def plot_confusion(cm, class_names, title):\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    plt.imshow(cm, interpolation=\"nearest\", cmap=\"Blues\")\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(class_names))\n",
        "    plt.xticks(tick_marks, class_names, rotation=45, ha=\"right\")\n",
        "    plt.yticks(tick_marks, class_names)\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"True\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n74zfodzKT_u",
        "outputId": "953069ef-bf4e-4f8d-e495-5f1e57bc1687"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================ FINAL RESULTS ================\n",
            "\n",
            "=== PATHMNIST INDIVIDUAL MODELS ===\n",
            "ResNet-18 : 0.5916434540389972 0.46720830040490086\n",
            "VGG-16    : 0.8064066852367688 0.7468896132710968\n",
            "ViT-Base  : 0.7987465181058496 0.7377623114521275\n",
            "DINO ViT  : 0.834958217270195 0.7762318564602073\n",
            "\n",
            "=== PATHMNIST ENSEMBLES ===\n",
            "Majority Voting : 0.8445682451253482 0.7771834821561808\n",
            "Weighted Avg    : 0.8682451253481894 0.8169716029008476\n",
            "Stacking        : 0.700974930362117 0.5683378180649454\n",
            "\n",
            "=== CHESTMNIST (ResNet-18 only) ===\n",
            "ResNet-18 : 0.6395934560691838 0.05572753013007492\n",
            "\n",
            "================ END METRICS ================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training and Evaluation Results**\n",
        "\n",
        "## PathMNIST — Individual Models\n",
        "\n",
        "| Model        | Accuracy | F1 Score |\n",
        "|--------------|----------|----------|\n",
        "| ResNet-18    | 0.592 | 0.467 |\n",
        "| VGG-16       | 0.806 | 0.747 |\n",
        "| ViT-Base     | 0.799 | 0.738 |\n",
        "| DINO ViT     | 0.835 | 0.776 |\n",
        "\n",
        "## PathMNIST — Ensemble Models\n",
        "\n",
        "| Ensemble Method      | Accuracy | F1 Score |\n",
        "|----------------------|----------|----------|\n",
        "| Majority Voting      | 0.845 | 0.777 |\n",
        "| Weighted Averaging   | **0.868** | **0.817** |\n",
        "| Stacking             | 0.701 | 0.568 |\n",
        "\n",
        "Weighted averaging performed the best overall.\n",
        "\n",
        "---\n",
        "\n",
        "## ChestMNIST — ResNet-18 Only\n",
        "\n",
        "| Model     | Accuracy | F1 Score |\n",
        "|-----------|----------|----------|\n",
        "| ResNet-18 | 0.640 | 0.056 |\n",
        "\n",
        "ChestMNIST is much harder due to being a multi-label dataset and heavily imbalanced.\n"
      ],
      "metadata": {
        "id": "ly6euf4x6Unx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion:\n",
        "# Comparison to MedMNIST Benchmarks\n",
        "\n",
        "The MedMNIST website reports AUC and ACC values for many models.\n",
        "\n",
        "## PathMNIST Benchmark Accuracy (ACC column)\n",
        "Most high-performing models reach around 0.90–0.91*accuracy.\n",
        "\n",
        "## My Best Model\n",
        "- Weighted Ensemble: 0.868 accuracy\n",
        "\n",
        "### Interpretation (simple terms)\n",
        "My model performs slightly below the highest reported accuracy.\n",
        "This is expected because:\n",
        "- I trained only one epoch\n",
        "- I used subset training instead of the full dataset\n",
        "- Colab limits GPU time\n",
        "\n",
        "Given these constraints, the performance is *ery close to benchmark-level models.\n",
        "\n",
        "## ChestMNIST Benchmark Accuracy\n",
        "Official accuracy is usually ~0.94.\n",
        "\n",
        "## My Result\n",
        "- 0.64 accuracy\n",
        "\n",
        "This is lower because ChestMNIST is originally multi-label. Collapsing labels into a single class loses important information. More training epochs would also help improve performance.\n",
        "\n",
        "# Individual models\n",
        " - ResNet-18 had the lowest accuracy - its a smaller network which may explain why it may not have determined as many useful features as the others.\n",
        " - VGG-16 and Vit-Base did well compared to ResNet-18 , and they are bigger models which naturally pick up more details from the images.\n",
        " - DINO ViT did the overall best here, which might be because it starts with pretrained features so even with a light fine tuning, it picks up the important details better.\n",
        " - Overall, the larger the models with stronger pretrained features did better here.\n",
        "\n",
        "# Ensamble methods :\n",
        "- Majority voting gave a small improvement compared to the single models.\n",
        "- Weighted averaging gave the best results overall - since it made the better models count more in the final predictions\n",
        "- combining the strengths of the models.\n",
        "- Stacking did overrl worse than the other ensambles, which makes sense since stacking tends to work best when training a seperate model on data it hasnt seen before, which isnt our set-up for this.\n",
        "\n"
      ],
      "metadata": {
        "id": "dq_Wl7h-SLfF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Challenges and Future Improvements\n",
        "\n",
        "## Challenges\n",
        "- Google Colab timeouts limited the number of training epochs.\n",
        "- Some models trained slowly, especially ViT and VGG.\n",
        "- ChestMNIST is a multi-label dataset, which doesn’t fit well with single-label training.\n",
        "- Reducing ChestMNIST to a single label hurts performance.\n",
        "\n",
        "## Future Improvements\n",
        "To match or beat the official leaderboard:\n",
        "\n",
        "1. Train for*more epochs\n",
        "2. Unfreeze more layers for deeper fine-tuning  \n",
        "3. Use the full datasets, not subsets  \n",
        "4. Treat ChestMNIST as a multi-label problem (sigmoid + BCE loss)  \n",
        "5. Use more data augmentation  \n",
        "6. Try stronger models (ResNet-50, EfficientNet, Swin Transformer)\n",
        "\n",
        "Even with constraints, the ensemble method clearly improved accuracy and showed why combining models is effective.\n"
      ],
      "metadata": {
        "id": "hlUddbf86-WD"
      }
    }
  ]
}